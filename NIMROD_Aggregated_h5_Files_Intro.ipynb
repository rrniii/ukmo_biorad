{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d985f60d-07fd-4f36-ba8b-ed3d5bf9be3a",
   "metadata": {},
   "source": [
    "!conda run -n pyart_3_8_radar_group python -m ipykernel install --user --name radar-env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dd4fc4-5da2-40b4-8221-0d0056c5975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pyart\n",
    "import netCDF4\n",
    "import h5py as h5\n",
    "from glob import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import cartopy.crs as ccrs\n",
    "import warnings \n",
    "import time \n",
    "import cartopy.feature as cfeature\n",
    "import cartopy.io.img_tiles as cimgt\n",
    "import matplotlib.ticker as mticker\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import rasterio\n",
    "import matplotlib\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881bd2a7-6cc1-46bb-8334-1bbc8d8e9a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file='/gws/nopw/j04/ncas_radar_vol3/ukmo-nimrod/raw_h5_data/single-site/chenies/2021/20210709_polar_pl_radar05_aggregate.h5'\n",
    "file='/gws/nopw/j04/ncas_radar_vol3/ukmo-nimrod/raw_h5_data/single-site/chenies/2025/20250816_polar_pl_radar05_aggregate.h5'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2318bdd-26c7-4f88-aa86-406dd65816c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5.File(file, 'r')\n",
    "\n",
    "#'lp' or 'sp'\n",
    "f['sp'].keys()\n",
    "\n",
    "#sp is every 10 minutes\n",
    "#lp is every 5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed3aaf2-b183-4ea7-abb1-0fb6071836b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f['sp'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2428304f-fd63-49bf-9658-0c61862dfe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f['lp']['0110'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c749e79c-1e12-4722-a445-e9af25627d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f['lp']['0110'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40d9087-67e0-4859-8e3e-a8333f58b3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "f['lp']['0110']['how'].attrs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d28caa1-e3e1-44ec-ab7d-ec926a7e8caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "f['lp']['0110']['what'].attrs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad0b609-7dcf-4303-9c34-c325454d044f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f['lp']['0110']['where'].attrs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d4bba1-18ea-4593-b2a3-e9e5a9283015",
   "metadata": {},
   "outputs": [],
   "source": [
    "f['lp']['0110']['dataset1'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8adb40b-9112-4d87-b684-1d1e50b9a1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "f['lp']['0110']['dataset1']['where'].attrs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd817f7d-c749-446c-8365-625de1cb8b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f['lp']['0110']['dataset1']['where'].attrs['rscale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5df18d-60bf-45ad-8c05-fb548f9c096e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f['lp']['0110']['dataset1']['where'].attrs['rscale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fc074d-0261-4853-9fe1-5e9ccd443f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "f['sp']['0110']['dataset1']['where'].attrs['rscale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e678ee6-883c-4b9e-84b8-7db43c309eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "f['lp']['1250']['dataset1']['how'].attrs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efd910d-6598-42f7-8e04-3e95be235810",
   "metadata": {},
   "outputs": [],
   "source": [
    "f['lp']['1250']['dataset1']['how'].attrs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dac3f55-591c-4b75-af0d-f720e9379d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f['lp']['1250']['dataset5']['how'].attrs['stopelA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135a69b5-72d7-407c-a4dc-3ed62180bd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "f['lp']['1250']['dataset1']['how'].attrs['stopazA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b6738d-b8c4-49ad-aef6-348818570544",
   "metadata": {},
   "outputs": [],
   "source": [
    "f['lp']['1250']['how'].attrs['pulsewidth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831f6251-2e22-4a17-a7d4-c5ec7b872aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "f['lp']['1250']['dataset1']['what'].attrs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee8ae53-1b01-4409-93a2-c199532bae87",
   "metadata": {},
   "outputs": [],
   "source": [
    "f['lp']['0110']['dataset1']['where'].attrs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5caacb4-2fc4-47e2-9610-6234be451756",
   "metadata": {},
   "outputs": [],
   "source": [
    "f['lp']['0110']['dataset1']['where'].attrs['rscale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c35165-7476-4d86-b5a4-205922a94979",
   "metadata": {},
   "outputs": [],
   "source": [
    "f['lp']['0110']['dataset1']['where'].attrs['nbins']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3fe5ad-6c5d-47e3-a69c-179eb9037bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "f['sp']['1250']['dataset1']['data1'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573912a6-d5ef-4517-8099-005571fbaf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "f['sp']['1250']['dataset1']['data1']['data'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cfbcfa-0d42-4e9a-92a2-67144315b93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f['sp']['1250']['dataset1']['data1']['what'].attrs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4f9f4b-4b05-4736-a1fb-c0a6906496ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pulse_selector='sp'\n",
    "data_selector='data1'\n",
    "print(f[pulse_selector]['0000']['dataset1'][data_selector]['what'].attrs['gain'])\n",
    "print(f[pulse_selector]['0000']['dataset1'][data_selector]['what'].attrs['offset'])\n",
    "print(f[pulse_selector]['0000']['dataset1'][data_selector]['what'].attrs['quantity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d69a88e-a225-4824-8c2b-4cb339d6fd43",
   "metadata": {},
   "source": [
    "# Function to Save Data in bioRad Compatible Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b35cd4-00a5-4463-9596-dc9129535b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import shutil\n",
    "\n",
    "import os\n",
    "#os.remove('test.h5')\n",
    "# Specify the file names and the dataset name\n",
    "#input_file = '/gws/nopw/j04/ncas_radar_vol3/ukmo-nimrod/raw_h5_data/single-site/chenies/2021/20210709_polar_pl_radar05_aggregate.h5'\n",
    "input_file='/work/scratch-pw4/rrniii/raw_h5_data_final/single-site/chenies/2025/20250720_polar_pl_radar05_aggregate.h5'\n",
    "output_file = '20250720_polar_pl_radar05_aggregate_lp_0000.h5'\n",
    "group_name = 'lp/1705/'\n",
    "dataset_name = '1705'\n",
    "\n",
    "# Open the existing H5 file in read mode\n",
    "with h5py.File(input_file, 'r') as h5file:\n",
    "    # Open the output H5 file in append mode ('a' mode ensures that existing data is preserved)\n",
    "    with h5py.File(output_file, 'w') as new_h5file:\n",
    "\n",
    "    # Datasets in selected type and time group\n",
    "    # Access the specific group by name\n",
    "        group = h5file[group_name]\n",
    "        # Function to recursively copy datasets to the root level\n",
    "        def copy_datasets_to_root(group, base_name=\"\"):\n",
    "            for item_name, item in group.items():\n",
    "                print('name is:'+item_name)\n",
    "                print(item)\n",
    "                full_item_name = f\"{base_name}{item_name}\"\n",
    "                if isinstance(item, h5py.Dataset):\n",
    "                    # Copy dataset to the root of the new file\n",
    "                    new_h5file.create_dataset(full_item_name, data=item[:])\n",
    "                elif isinstance(item, h5py.Group):\n",
    "                    if item_name=='where' or 'what':\n",
    "                        h5file.copy(h5file[group_name+full_item_name], new_h5file, name=full_item_name)\n",
    "                        \n",
    "                    # Recursively copy items in the sub-group, flattening the hierarchy\n",
    "                    else:\n",
    "                        copy_datasets_to_root(item, base_name=f\"{full_item_name}/\")\n",
    "    \n",
    "        # Start copying from the specified group\n",
    "        copy_datasets_to_root(group)\n",
    "\n",
    "    print(f\"Datasets from group '{group_name}' and root attributes have been saved directly to '{output_file}' without the group hierarchy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f090f85-0761-4133-9925-b12f225e38e9",
   "metadata": {},
   "source": [
    "# Function to Read NIMROD Aggregated Files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a11fb08-83bd-41cf-9bd3-d0e53881e343",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Routines for reading NIMROD ODIM_H5 Aggregated files.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "try:\n",
    "    import h5py\n",
    "    _H5PY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    _H5PY_AVAILABLE = False\n",
    "\n",
    "from pyart.config import FileMetadata, get_fillvalue\n",
    "from pyart.io.common import make_time_unit_str, _test_arguments\n",
    "from pyart.core.radar import Radar\n",
    "from pyart.exceptions import MissingOptionalDependency\n",
    "\n",
    "\n",
    "ODIM_H5_FIELD_NAMES = {\n",
    "    'TH': 'total_power',        # uncorrected reflectivity, horizontal\n",
    "    'TV': 'total_power',        # uncorrected reflectivity, vertical\n",
    "    'DBZH': 'reflectivity',     # corrected reflectivity, horizontal\n",
    "    'DBZV': 'reflectivity',     # corrected reflectivity, vertical\n",
    "    'ZDR': 'differential_reflectivity',     # differential reflectivity\n",
    "    'RHOHV': 'cross_correlation_ratio',\n",
    "    'LDR': 'linear_polarization_ratio',\n",
    "    'PHIDP': 'differential_phase',\n",
    "    'KDP': 'specific_differential_phase',\n",
    "    'SQI': 'normalized_coherent_power',\n",
    "    'SNR': 'signal_to_noise_ratio',\n",
    "    'VRAD': 'velocity', # radial velocity, marked for deprecation in ODIM HDF5 2.2\n",
    "    'VRADH': 'velocity', # radial velocity, horizontal polarisation\n",
    "    'VRADV': 'velocity', # radial velocity, vertical polarisation\n",
    "    'WRAD': 'spectrum_width',\n",
    "    'QIND': 'quality_index',\n",
    "    'CI': 'clutter_index', #Add for NIMROD files\n",
    "    'LONG_RANGE_NOISE_DBC_H': 'LONG_RANGE_NOISE_DBC_H', #Add for NIMROD files\n",
    "    'LONG_RANGE_NOISE_DBC_V': 'LONG_RANGE_NOISE_DBC_V', #Add for NIMROD files\n",
    "}\n",
    "\n",
    "\n",
    "def read_nimrod_aggregated_odim_h5(filename,data_type, time, field_names=None, additional_metadata=None,\n",
    "                 file_field_names=False, exclude_fields=None,\n",
    "                 include_fields=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Read a NIMRD ODIM_H5 Aggregated file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        Name of the ODIM_H5 file to read.\n",
    "    data_type : str\n",
    "        lp (long pulse) or sp (short pulse) data to be read  #Need to expand to read vertical\n",
    "    time : str\n",
    "        string with the format hhmm that split the day into 144 ten minute chunks for sp data \n",
    "        or 288 5 minute chunks for lp data from 0000 to 2350. \n",
    "        Each period has been aggregated to make single volume.   \n",
    "    field_names : dict, optional\n",
    "        Dictionary mapping ODIM_H5 field names to radar field names. If a\n",
    "        data type found in the file does not appear in this dictionary or has\n",
    "        a value of None it will not be placed in the radar.fields dictionary.\n",
    "        A value of None, the default, will use the mapping defined in the\n",
    "        Py-ART configuration file.\n",
    "    additional_metadata : dict of dicts, optional\n",
    "        Dictionary of dictionaries to retrieve metadata from during this read.\n",
    "        This metadata is not used during any successive file reads unless\n",
    "        explicitly included.  A value of None, the default, will not\n",
    "        introduct any addition metadata and the file specific or default\n",
    "        metadata as specified by the Py-ART configuration file will be used.\n",
    "    file_field_names : bool, optional\n",
    "        True to use the MDV data type names for the field names. If this\n",
    "        case the field_names parameter is ignored. The field dictionary will\n",
    "        likely only have a 'data' key, unless the fields are defined in\n",
    "        `additional_metadata`.\n",
    "    exclude_fields : list or None, optional\n",
    "        List of fields to exclude from the radar object. This is applied\n",
    "        after the `file_field_names` and `field_names` parameters. Set\n",
    "        to None to include all fields specified by include_fields.\n",
    "    include_fields : list or None, optional\n",
    "        List of fields to include from the radar object. This is applied\n",
    "        after the `file_field_names` and `field_names` parameters. Set\n",
    "        to None to include all fields not specified by exclude_fields.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    radar : Radar\n",
    "        Radar object containing data from ODIM_H5 file.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO before moving to pyart.io\n",
    "    # * unit test\n",
    "    # * add default field mapping, etc to default config\n",
    "    # * auto-detect file type with pyart.io.read function\n",
    "    # * instrument parameters\n",
    "    # * add additional checks for HOW attributes\n",
    "    # * support for other objects (SCAN, XSEC)\n",
    "\n",
    "    # check that h5py is available\n",
    "    if not _H5PY_AVAILABLE:\n",
    "        raise MissingOptionalDependency(\n",
    "            \"h5py is required to use read_odim_h5 but is not installed\")\n",
    "\n",
    "    # test for non empty kwargs\n",
    "    _test_arguments(kwargs)\n",
    "\n",
    "    # create metadata retrieval object\n",
    "    if field_names is None:\n",
    "        field_names = ODIM_H5_FIELD_NAMES\n",
    "    filemetadata = FileMetadata('odim_h5', field_names, additional_metadata,\n",
    "                                file_field_names, exclude_fields,\n",
    "                                include_fields)\n",
    "\n",
    "    # open the file\n",
    "    with h5py.File(filename, 'r') as hfile:\n",
    "        hfile=hfile[data_type][time]\n",
    "        odim_object = _to_str(hfile['what'].attrs['object'])\n",
    "        if odim_object not in ['PVOL', 'SCAN', 'ELEV', 'AZIM']:\n",
    "            raise NotImplementedError(\n",
    "                'object: %s not implemented.' % (odim_object))\n",
    "\n",
    "        # determine the number of sweeps by the number of groups which\n",
    "        # begin with dataset\n",
    "        datasets = [k for k in hfile if k.startswith('dataset')]\n",
    "        datasets.sort(key=lambda x: int(x[7:]))\n",
    "        \n",
    "        #Added to remove vert scans from NIMROD aggregated data as they are a different resolution\n",
    "        for d in datasets:\n",
    "            if hfile[d]['where'].attrs['elangle']>89.0:\n",
    "                datasets.remove(d)\n",
    "        \n",
    "        \n",
    "        nsweeps = len(datasets)\n",
    "\n",
    "        # latitude, longitude and altitude\n",
    "        latitude = filemetadata('latitude')\n",
    "        longitude = filemetadata('longitude')\n",
    "        altitude = filemetadata('altitude')\n",
    "\n",
    "        h_where = hfile['where'].attrs\n",
    "        latitude['data'] = np.array([h_where['lat']], dtype='float64')\n",
    "        longitude['data'] = np.array([h_where['lon']], dtype='float64')\n",
    "        altitude['data'] = np.array([h_where['height']], dtype='float64')\n",
    "\n",
    "        # metadata\n",
    "        metadata = filemetadata('metadata')\n",
    "        metadata['instrument_name']=_get_radar_name_from_radar_numer(hfile['what'].attrs['source_local_site_number'])\n",
    "        metadata['source'] = _to_str(hfile['what'].attrs['source'])\n",
    "        metadata['original_container'] = 'ukmo_nimrod_aggregated_odim_h5'\n",
    "        metadata['odim_conventions'] = _to_str(hfile.attrs['Conventions'])\n",
    "\n",
    "        h_what = hfile['what'].attrs\n",
    "        metadata['version'] = 'nimrod_test'#_to_str(h_what['version']) ####Need to talk to josh about it\n",
    "        metadata['source'] = _to_str(h_what['source'])\n",
    "\n",
    "        try:\n",
    "            ds1_how = hfile[datasets[0]]['how'].attrs\n",
    "        except KeyError:\n",
    "            # if no how group exists mock it with an empty dictionary\n",
    "            ds1_how = {}\n",
    "        if 'system' in ds1_how:\n",
    "            metadata['system'] = ds1_how['system']\n",
    "        if 'software' in ds1_how:\n",
    "            metadata['software'] = ds1_how['software']\n",
    "        if 'sw_version' in ds1_how:\n",
    "            metadata['sw_version'] = ds1_how['sw_version']\n",
    "\n",
    "        # sweep_start_ray_index, sweep_end_ray_index\n",
    "        sweep_start_ray_index = filemetadata('sweep_start_ray_index')\n",
    "        sweep_end_ray_index = filemetadata('sweep_end_ray_index')\n",
    "\n",
    "        if odim_object in ['AZIM', 'SCAN', 'PVOL']:\n",
    "            rays_per_sweep = [\n",
    "                int(hfile[d]['where'].attrs['nrays']) for d in datasets]\n",
    "        elif odim_object == 'ELEV':\n",
    "            rays_per_sweep = [\n",
    "                int(hfile[d]['where'].attrs['angles'].size) for d in datasets]\n",
    "        total_rays = sum(rays_per_sweep)\n",
    "        ssri = np.cumsum(np.append([0], rays_per_sweep[:-1])).astype('int32')\n",
    "        seri = np.cumsum(rays_per_sweep).astype('int32') - 1\n",
    "        sweep_start_ray_index['data'] = ssri\n",
    "        sweep_end_ray_index['data'] = seri\n",
    "\n",
    "        # sweep_number\n",
    "        sweep_number = filemetadata('sweep_number')\n",
    "        sweep_number['data'] = np.arange(nsweeps, dtype='int32')\n",
    "\n",
    "        # sweep_mode\n",
    "        sweep_mode = filemetadata('sweep_mode')\n",
    "        sweep_mode['data'] = np.array(nsweeps * ['azimuth_surveillance'])\n",
    "\n",
    "        # scan_type\n",
    "        if odim_object == 'ELEV':\n",
    "            scan_type = 'rhi'\n",
    "        else:\n",
    "            scan_type = 'ppi'\n",
    "\n",
    "        # fixed_angle\n",
    "        fixed_angle = filemetadata('fixed_angle')\n",
    "        if odim_object == 'ELEV':\n",
    "            sweep_el = [hfile[d]['where'].attrs['az_angle'] for d in datasets]\n",
    "        else:\n",
    "            sweep_el = [hfile[d]['where'].attrs['elangle'] for d in datasets]\n",
    "        fixed_angle['data'] = np.array(sweep_el, dtype='float32')\n",
    "\n",
    "        # elevation\n",
    "        elevation = filemetadata('elevation')\n",
    "        if 'elangles' in ds1_how:\n",
    "            edata = np.empty(total_rays, dtype='float32')\n",
    "            for d, start, stop in zip(datasets, ssri, seri):\n",
    "                edata[start:stop+1] = hfile[d]['how'].attrs['elangles'][:]\n",
    "            elevation['data'] = edata\n",
    "        elif odim_object == 'ELEV':\n",
    "            edata = np.empty(total_rays, dtype='float32')\n",
    "            for d, start, stop in zip(datasets, ssri, seri):\n",
    "                edata[start:stop+1] = hfile[d]['where'].attrs['angles'][:]\n",
    "            elevation['data'] = edata\n",
    "        else:\n",
    "            elevation['data'] = np.repeat(sweep_el, rays_per_sweep)\n",
    "\n",
    "        # range\n",
    "        _range = filemetadata('range')\n",
    "        if 'rstart' in hfile['dataset1/where'].attrs:\n",
    "            # derive range from rstart and rscale attributes if available\n",
    "\n",
    "            # check that the gate spacing is constant between sweeps\n",
    "            rstart = [hfile[d]['where'].attrs['rstart'] for d in datasets]\n",
    "            if any(rstart != rstart[0]):\n",
    "                raise ValueError('range start changes between sweeps')\n",
    "            rscale = [hfile[d]['where'].attrs['rscale'] for d in datasets]\n",
    "            if any(rscale != rscale[0]):\n",
    "                raise ValueError('range scale changes between sweeps')\n",
    "            all_sweeps_nbins = [hfile[d]['where'].attrs['nbins'] for d in datasets]\n",
    "            # check for max range off all sweeps\n",
    "            max_nbins = max(all_sweeps_nbins)\n",
    "\n",
    "            if isinstance(max_nbins, np.ndarray):\n",
    "                max_nbins = max_nbins[0]\n",
    "            else:\n",
    "                max_nbins = max(all_sweeps_nbins)\n",
    "\n",
    "            rscenter = 1e3 * rstart[0] + rscale[0] / 2\n",
    "            _range['data'] = np.arange(rscenter,\n",
    "                                       rscenter + max_nbins * rscale[0],\n",
    "                                       rscale[0], dtype='float32')\n",
    "            _range['meters_to_center_of_first_gate'] = rstart[0] * 1000.\n",
    "            _range['meters_between_gates'] = float(rscale[0])\n",
    "        else:\n",
    "            # if not defined use range attribute which defines the maximum range\n",
    "            # in km. There is no information on the starting location of the\n",
    "            # range bins so we assume this to be 0.\n",
    "            # This most often occurs in RHI files, which technically do not meet\n",
    "            # the ODIM 2.2 specs. Section 7.4 requires that these files include\n",
    "            # the where/rstart, where/rscale and where/nbins attributes.\n",
    "            max_range = [hfile[d]['where'].attrs['range'] for d in datasets]\n",
    "            if any(max_range != max_range[0]):\n",
    "                raise ValueError('maximum range changes between sweeps')\n",
    "            # nbins is required\n",
    "            nbins = hfile['dataset1/data1/data'].shape[1]\n",
    "            _range['data'] = np.linspace(\n",
    "                0, max_range[0] * 1000., nbins).astype('float32')\n",
    "            _range['meters_to_center_of_first_gate'] = 0\n",
    "            _range['meters_between_gates'] = max_range[0] * 1000. / nbins\n",
    "\n",
    "        # azimuth\n",
    "        azimuth = filemetadata('azimuth')\n",
    "        az_data = np.ones((total_rays, ), dtype='float32')\n",
    "        for dset, start, stop in zip(datasets, ssri, seri):\n",
    "            if odim_object == 'ELEV':\n",
    "                # all azimuth angles are the sweep azimuth angle\n",
    "                sweep_az = hfile[dset]['where'].attrs['az_angle']\n",
    "            elif odim_object == 'AZIM':\n",
    "                # Sector azimuths are specified in the startaz and stopaz\n",
    "                # attribute of dataset/where.\n",
    "                # Assume that the azimuth angles do not pass through 0/360 deg.\n",
    "                startaz = hfile[dset]['where'].attrs['startaz']\n",
    "                stopaz = hfile[dset]['where'].attrs['stopaz']\n",
    "                nrays = stop - start + 1\n",
    "                sweep_az = np.linspace(startaz, stopaz, nrays, endpoint=True)\n",
    "            elif ('startazA' in ds1_how) and ('stopazA' in ds1_how):\n",
    "                # average between start and stop azimuth angles\n",
    "                startaz = hfile[dset]['how'].attrs['startazA']\n",
    "                stopaz = hfile[dset]['how'].attrs['stopazA']\n",
    "                sweep_az = np.angle(\n",
    "                    (np.exp(1.j*np.deg2rad(startaz)) +\n",
    "                    np.exp(1.j*np.deg2rad(stopaz))) / 2., deg=True)\n",
    "            else:\n",
    "                # according to section 5.1 the first ray points to the\n",
    "                # northernmost direction and proceeds clockwise for a complete\n",
    "                # 360 rotation.\n",
    "                try:\n",
    "                    astart = hfile[dset]['how'].attrs['astart']\n",
    "                except KeyError:\n",
    "                    astart = 0.0\n",
    "                nrays = hfile[dset]['where'].attrs['nrays']\n",
    "                da = 360.0 / nrays\n",
    "                sweep_az = np.arange(astart + da / 2., 360., da, dtype='float32')\n",
    "            az_data[start:stop+1] = sweep_az\n",
    "        azimuth['data'] = az_data\n",
    "\n",
    "        # time\n",
    "        _time = filemetadata('time')\n",
    "        if ('startazT' in ds1_how) and ('stopazT' in ds1_how):\n",
    "            # average between startazT and stopazT\n",
    "            t_data = np.empty((total_rays, ), dtype='float32')\n",
    "            for dset, start, stop in zip(datasets, ssri, seri):\n",
    "                t_start = hfile[dset]['how'].attrs['startazT']\n",
    "                t_stop = hfile[dset]['how'].attrs['stopazT']\n",
    "                t_data[start:stop+1] = (t_start + t_stop) / 2\n",
    "            start_epoch = t_data.min()\n",
    "            start_time = datetime.datetime.utcfromtimestamp(start_epoch)\n",
    "            _time['units'] = make_time_unit_str(start_time)\n",
    "            _time['data'] = t_data - start_epoch\n",
    "        else:\n",
    "            t_data = np.empty((total_rays, ), dtype='int32')\n",
    "            # interpolate between each sweep starting and ending time\n",
    "            for dset, start, stop in zip(datasets, ssri, seri):\n",
    "                dset_what = hfile[dset]['what'].attrs\n",
    "                start_str = _to_str(\n",
    "                    dset_what['startdate'] + dset_what['starttime'])\n",
    "                end_str = _to_str(dset_what['enddate'] + dset_what['endtime'])\n",
    "                start_dt = datetime.datetime.strptime(start_str, '%Y%m%d%H%M%S')\n",
    "                end_dt = datetime.datetime.strptime(end_str, '%Y%m%d%H%M%S')\n",
    "\n",
    "                time_delta = end_dt - start_dt\n",
    "                delta_seconds = time_delta.seconds + time_delta.days * 3600 * 24\n",
    "                rays = stop - start + 1\n",
    "                sweep_start_epoch = (\n",
    "                    start_dt - datetime.datetime(1970, 1, 1)).total_seconds()\n",
    "                t_data[start:stop+1] = (sweep_start_epoch +\n",
    "                                        np.linspace(0, delta_seconds, rays))\n",
    "            start_epoch = t_data.min()\n",
    "            start_time = datetime.datetime.utcfromtimestamp(start_epoch)\n",
    "            _time['units'] = make_time_unit_str(start_time)\n",
    "            _time['data'] = (t_data - start_epoch).astype('float32')\n",
    "\n",
    "        # fields\n",
    "        fields = {}\n",
    "        h_field_keys = [k for k in hfile['dataset1'] if k.startswith('data')]\n",
    "        odim_fields = [hfile['dataset1'][d]['what'].attrs['quantity'] for d in h_field_keys]\n",
    "        for odim_field, h_field_key in zip(odim_fields, h_field_keys):\n",
    "            field_name = filemetadata.get_field_name(_to_str(odim_field))\n",
    "            if field_name is None:\n",
    "                continue\n",
    "            fdata = np.ma.zeros((total_rays, max_nbins), dtype='float32')\n",
    "            start = 0\n",
    "            # loop on the sweeps, copy data into correct location in data array\n",
    "            for dset, rays_in_sweep in zip(datasets, rays_per_sweep):\n",
    "                try:\n",
    "                    sweep_data = _get_odim_h5_sweep_data(hfile[dset][h_field_key])\n",
    "                except KeyError:\n",
    "                    sweep_data = np.zeros((rays_in_sweep, max_nbins)) + np.NaN\n",
    "                sweep_nbins = sweep_data.shape[1]\n",
    "                fdata[start:start + rays_in_sweep, :sweep_nbins] = sweep_data[:]\n",
    "                # set data to NaN if its beyond the range of this sweep\n",
    "                fdata[start:start + rays_in_sweep, sweep_nbins:max_nbins] = np.nan\n",
    "                start += rays_in_sweep\n",
    "            # create field dictionary\n",
    "            field_dic = filemetadata(field_name)\n",
    "            field_dic['data'] = fdata\n",
    "            field_dic['_FillValue'] = get_fillvalue()\n",
    "            fields[field_name] = field_dic\n",
    "\n",
    "    # instrument_parameters\n",
    "    instrument_parameters = None\n",
    "\n",
    "    return Radar(\n",
    "        _time, _range, fields, metadata, scan_type,\n",
    "        latitude, longitude, altitude,\n",
    "        sweep_number, sweep_mode, fixed_angle, sweep_start_ray_index,\n",
    "        sweep_end_ray_index,\n",
    "        azimuth, elevation,\n",
    "        instrument_parameters=instrument_parameters)\n",
    "\n",
    "\n",
    "\n",
    "def _to_str(text):\n",
    "    \"\"\" Convert bytes to str if necessary. \"\"\"\n",
    "    if hasattr(text, 'decode'):\n",
    "        return text.decode('utf-8')\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "\n",
    "def _get_odim_h5_sweep_data(group):\n",
    "    \"\"\" Get ODIM_H5 sweet data from an HDF5 group. \"\"\"\n",
    "\n",
    "    # mask raw data\n",
    "    what = group['what']\n",
    "    raw_data = group['data'][:]\n",
    "\n",
    "    if 'nodata' in what.attrs:\n",
    "        nodata = what.attrs.get('nodata')\n",
    "        data = np.ma.masked_equal(raw_data, nodata)\n",
    "    else:\n",
    "        data = np.ma.masked_array(raw_data)\n",
    "    if 'undetect' in what.attrs:\n",
    "        undetect = what.attrs.get('undetect')\n",
    "        data[data == undetect] = np.ma.masked\n",
    "        \n",
    "    offset = 0.0\n",
    "    gain = 1.0\n",
    "    if 'offset' in what.attrs:\n",
    "        offset = what.attrs.get('offset')\n",
    "    if 'gain' in what.attrs:\n",
    "        gain = what.attrs.get('gain')\n",
    "    return data * gain + offset\n",
    "\n",
    "def _get_radar_name_from_radar_numer(radar_number):\n",
    "    if radar_number==7:\n",
    "        radar_name='Castor Bay'\n",
    "    if radar_number==5:\n",
    "        radar_name='Chenies'\n",
    "    if radar_number==3:\n",
    "        radar_name='Clee Hill'\n",
    "    if radar_number==16:\n",
    "        radar_name='Cobbacomebe Cross'\n",
    "    if radar_number==10:\n",
    "        radar_name='Crug-y-gorrllwyn'\n",
    "    if radar_number==21:\n",
    "        radar_name='Dean Hill'\n",
    "    if radar_number==15:\n",
    "        radar_name='Druim a\\'Starraig'\n",
    "    if radar_number==14:\n",
    "        radar_name='Dudwick'\n",
    "    if radar_number==4:\n",
    "        radar_name='Hameldon Hill'\n",
    "    if radar_number==23:\n",
    "        radar_name='High Moorsley'\n",
    "    if radar_number==18:\n",
    "        radar_name='Holehead'\n",
    "    if radar_number==9:\n",
    "        radar_name='Ingham'\n",
    "    if radar_number==12:\n",
    "        radar_name='Jersey'\n",
    "    if radar_number==19:\n",
    "        radar_name='Munduff Hill'\n",
    "    if radar_number==8:\n",
    "        radar_name='Predannack'\n",
    "    if radar_number==20:\n",
    "        radar_name='Thurnham'\n",
    "    if radar_number==11:\n",
    "        radar_name='Wardon-hill'\n",
    "    \n",
    "    return radar_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f7fef8-5b82-4596-b479-232b1ce3aed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "time='1700'\n",
    "data_type='sp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4feffe3a-9048-4a5f-8962-b69d028c9aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "radar=read_nimrod_aggregated_odim_h5(file,data_type, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3ac639-ce36-499c-8f8b-0b9875f03b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded=np.where(np.logical_or((radar.fields['cross_correlation_ratio']['data'].data<0.2),(radar.fields['cross_correlation_ratio']['data'].data>0.9)))\n",
    "radar.fields['cross_correlation_ratio']['data'].data[excluded]=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a610c078-29aa-4fbe-bedf-ee9020991d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "radar.fields['cross_correlation_ratio']['data'].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893d583c-d623-4831-8479-d863279fed44",
   "metadata": {},
   "source": [
    "# Plotting Function that uses the reading function above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea4a414-427a-4d94-8c1f-7820ee435538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ukmo_radar(fpath,data_type,time ,R,ele,plotpath, filter=False):\n",
    "#Plotting Options Set in Function\n",
    "#R = 50#  #Min and Mac Range from Radar in km\n",
    "\n",
    "\n",
    "    #Read Files and Setup Plotting\n",
    "    radar=read_nimrod_aggregated_odim_h5(fpath,data_type, time)\n",
    "    if filter==True:\n",
    "        alts=[]\n",
    "        for e, e_val in enumerate(radar.fixed_angle['data']):\n",
    "            lat, lon, alt= radar.get_gate_lat_lon_alt(e)\n",
    "            if e==0:\n",
    "                alts=alt\n",
    "            else:\n",
    "                alts=np.append(alts,alt,axis=0)\n",
    "        excluded=np.where(np.logical_or(np.logical_or(np.logical_or(np.logical_or(\n",
    "            np.logical_or((radar.fields['cross_correlation_ratio']['data'].data<0.25),(radar.fields['cross_correlation_ratio']['data'].data>0.9)),\n",
    "            radar.fields['normalized_coherent_power']['data'].data<0.1),\n",
    "            alts>2500),\n",
    "            (radar.fields['differential_phase']['data'].data<5)),\n",
    "            (radar.fields['normalized_coherent_power']['data'].data>0.8)) \n",
    "            )\n",
    "        radar.fields['reflectivity']['data'].data[excluded]=np.nan\n",
    "        radar.fields['differential_reflectivity']['data'].data[excluded]=np.nan\n",
    "        radar.fields['cross_correlation_ratio']['data'].data[excluded]=np.nan\n",
    "        radar.fields['differential_phase']['data'].data[excluded]=np.nan\n",
    "        radar.fields['velocity']['data'].data[excluded]=np.nan\n",
    "        radar.fields['normalized_coherent_power']['data'].data[excluded]=np.nan\n",
    "    \n",
    "    display = pyart.graph.RadarDisplay(radar)\n",
    "\n",
    "    # set the figure title and show\n",
    "    instrument_name = radar.metadata['instrument_name']\n",
    "    time_start = netCDF4.num2date(radar.time['data'][0], radar.time['units'])\n",
    "    time_text = ' ' + time_start.strftime('%Y-%m-%d %H:%M:%SZ')\n",
    "\n",
    "    elevation = radar.fixed_angle['data'][ele]\n",
    "    ele_text= '%0.1f' % (elevation)\n",
    "\n",
    "    title = instrument_name + time_text + ' Elevation: '+ str(elevation) + ' Scan Type: '+data_type\n",
    "\n",
    " \n",
    "    \n",
    "    if np.round(elevation,0)>9.99:\n",
    "        ele_folder ='ele'+ele_text[0]+ele_text[1]+'_'+ele_text[3]+ '/'\n",
    "        ele_name= 'ele'+ele_text[0]+ele_text[1]+'_'+ele_text[3]\n",
    "    else:\n",
    "        ele_folder ='ele'+ele_text[0]+'_'+ele_text[2]+ '/'\n",
    "        ele_name= 'ele'+ele_text[0]+'_'+ele_text[2]\n",
    "        \n",
    "    plot_file_name = instrument_name +'_'+ time_start.strftime('%Y-%m-%dT%H%M%SZ')+'_' +ele_name+'_'+data_type+ '.png'\n",
    "\n",
    "    loc_folder = instrument_name+ '/'\n",
    "    \n",
    "    R_folder = str(R)+'km'+ '/'\n",
    "\n",
    "    fig_save_file_path = plotpath+loc_folder+ ele_folder + R_folder\n",
    "\n",
    "    #Figure Options\n",
    "    width=15 #in inches\n",
    "    height=7.5 #in inches\n",
    "\n",
    "    fig = plt.figure(figsize=(width, height))\n",
    "    nrows=2\n",
    "    ncols=3\n",
    "\n",
    "    ax1 = fig.add_subplot(nrows,ncols,1)\n",
    "    display.plot('reflectivity',ele,ax=ax1, vmin=-32, vmax=45., title='Horizontal Reflectivity', colorbar_label=radar.fields['reflectivity']['units'],\n",
    "                 axislabels=('', 'North South distance from radar (km)'))\n",
    "    display.set_limits((-R, R), (-R, R), ax=ax1)\n",
    "\n",
    "    \n",
    "    ax2 = fig.add_subplot(nrows,ncols,2)\n",
    "    display.plot('differential_reflectivity', ele,ax=ax2, vmin=-2, vmax=10., title='Differential Reflectivity', colorbar_label=radar.fields['differential_reflectivity']['units'],\n",
    "                 axislabels=('', ''), cmap = 'pyart_RefDiff')\n",
    "    display.set_limits((-R, R), (-R, R), ax=ax2)\n",
    "\n",
    "\n",
    "    ax3 = fig.add_subplot(nrows,ncols,3)\n",
    "    display.plot('cross_correlation_ratio', ele,ax=ax3, vmin=0.0, vmax=1.0, title='Cross Correlation Ratio', colorbar_label=radar.fields['cross_correlation_ratio']['units'],\n",
    "                 axislabels=('', ''), cmap = 'pyart_RefDiff')\n",
    "    display.set_limits((-R, R), (-R, R), ax=ax3)\n",
    "\n",
    "    \n",
    "    ax4 = fig.add_subplot(nrows,ncols,4)\n",
    "    display.plot('velocity',ele,ax=ax4, vmin=-2, vmax=2., title='Doppler Velocity', colorbar_label=radar.fields['velocity']['units'],\n",
    "                 axislabels=('East West distance from radar (km)', 'North South distance from radar (km)'), cmap = 'pyart_BuDRd18')\n",
    "    display.set_limits((-R, R), (-R, R), ax=ax4)\n",
    "    \n",
    "    \n",
    "    ax5 = fig.add_subplot(nrows,ncols,5)\n",
    "    display.plot('differential_phase', ele,ax=ax5, vmin=-5, vmax=120., title='Differential Phase', colorbar_label=radar.fields['differential_phase']['units'],\n",
    "                 axislabels=('East West distance from radar (km)', ''), cmap = 'pyart_Wild25')\n",
    "    display.set_limits((-R, R), (-R, R), ax=ax5)\n",
    "\n",
    "    \n",
    "    #ax6 = fig.add_subplot(nrows,ncols,6)\n",
    "    #display.plot('spectrum_width', ele, ax=ax6, vmin=0, vmax=5., title='Spectrum Width', colorbar_label=radar.fields['spectrum_width']['units'],\n",
    "    #             axislabels=('East West distance from radar (km)', ''), cmap = 'pyart_NWS_SPW')\n",
    "    #display.set_limits((-R, R), (-R, R), ax=ax6)\n",
    "\n",
    "    \n",
    "    ax6 = fig.add_subplot(nrows,ncols,6)\n",
    "    display.plot('normalized_coherent_power', ele, ax=ax6, title='Normalized Coherent Power', colorbar_label=radar.fields['normalized_coherent_power']['units'],\n",
    "                 axislabels=('East West distance from radar (km)', ''), cmap = 'pyart_Carbone17')\n",
    "    \n",
    "    display.set_limits((-R, R), (-R, R), ax=ax6)\n",
    "\n",
    "    plt.suptitle(title, fontsize=24)\n",
    "    \n",
    "    \n",
    "    if not os.path.exists(fig_save_file_path):\n",
    "        os.makedirs(fig_save_file_path)\n",
    "    if filter==True:\n",
    "        plot_file_name=plot_file_name[0:-3]+'_filtered.png'\n",
    "    plt.savefig(fig_save_file_path+plot_file_name, dpi=100)\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    del radar\n",
    "    del display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14641e3-6c90-4b5a-8726-d01f77712a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "time='1200'\n",
    "data_type='lp'\n",
    "R=50\n",
    "Wele=4\n",
    "plotpath='~/test'\n",
    "filter=[True, False]\n",
    "ele=[0,1,2,3,4]\n",
    "for f in filter:\n",
    "    for e in ele:\n",
    "        plot_ukmo_radar(file,data_type,time ,R,e,plotpath,filter=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c816f92-5d94-4234-942e-0a9b991d908f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f237ec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e2ac02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575eb74f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyart_3_8_radar_group",
   "language": "python",
   "name": "pyart_3_8_radar_group"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
